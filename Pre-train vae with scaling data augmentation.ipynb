{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils.vmf_batch import vMF\n",
    "from datetime import datetime\n",
    "from importlib import reload\n",
    "import os\n",
    "\n",
    "from models import SeqEncoder, SeqDecoder, Seq2Seq_VAE, PoolingClassifier, init_weights\n",
    "from itertools import product\n",
    "from training_utils import  train, evaluate, scale\n",
    "\n",
    "\n",
    "## plotting ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MASKING_ELEMENT =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '5_populations'\n",
    "with open('./data/toy_data/%s/iterator/scaling_val_iterator.pkl'%folder, 'rb') as f:\n",
    "    val_iterator = pickle.load(f)\n",
    "\n",
    "with open('./data/toy_data/%s/iterator/scaling_train_iterator.pkl'%folder, 'rb') as f:\n",
    "    train_iterator = pickle.load(f)\n",
    "    \n",
    "N_train = len(train_iterator.sampler.indices)\n",
    "N_val = len(val_iterator.sampler.indices)\n",
    "n_walks = train_iterator.dataset.n_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_loss(x, reconstructed_x, ignore_el=MASKING_ELEMENT):\n",
    "    # reconstruction loss\n",
    "    # x = [trg len, batch size * n walks, output dim]\n",
    "\n",
    "    seq_len , bs, output_dim = x.shape\n",
    "    mask = x[:,:,0] != ignore_el\n",
    "    RCL = 0\n",
    "    for d in range(output_dim):\n",
    "        RCL += mse_loss(reconstructed_x[:,:,d][mask], x[:,:,d][mask])\n",
    "    RCL /= output_dim\n",
    "    \n",
    "    return RCL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "INPUT_DIM = 3   \n",
    "lr = 1e-2                           # learning rate\n",
    "NUM_LAYERS = 2\n",
    "NUM_CLASSES = 5#3\n",
    "N_EPOCHS = 150\n",
    "MASKING_ELEMENT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD: 45.709938049316406\n",
      "Epoch 0, Train Loss: 3827.649593750, Test Loss: 16823.017500000\n",
      "Epoch 1, Train Loss: 2891.111500000, Test Loss: 20616.311000000\n",
      "Epoch 2, Train Loss: 2295.264265625, Test Loss: 27830.733000000\n",
      "Epoch 3, Train Loss: 1985.287640625, Test Loss: 28018.933000000\n",
      "Epoch 4, Train Loss: 1868.439203125, Test Loss: 26623.088000000\n",
      "Epoch 5, Train Loss: 1808.230531250, Test Loss: 15454.372500000\n",
      "Epoch 6, Train Loss: 1757.370515625, Test Loss: 18782.912000000\n",
      "Epoch 7, Train Loss: 1710.964843750, Test Loss: 17382.072000000\n",
      "Epoch 8, Train Loss: 1696.632718750, Test Loss: 17250.768000000\n",
      "Epoch 9, Train Loss: 1657.499859375, Test Loss: 15459.180000000\n",
      "Epoch 10, Train Loss: 1650.165625000, Test Loss: 13742.372000000\n",
      "Epoch 11, Train Loss: 1629.673281250, Test Loss: 18179.383000000\n",
      "Epoch 12, Train Loss: 1612.713671875, Test Loss: 16148.770500000\n",
      "Epoch 13, Train Loss: 1563.746187500, Test Loss: 16194.820500000\n",
      "Epoch 14, Train Loss: 1552.856812500, Test Loss: 10964.830000000\n",
      "Epoch 15, Train Loss: 1523.546343750, Test Loss: 12841.495000000\n",
      "Epoch 16, Train Loss: 1466.866031250, Test Loss: 12877.195000000\n",
      "Epoch 17, Train Loss: 1406.699546875, Test Loss: 8694.133250000\n",
      "Epoch 18, Train Loss: 1351.261265625, Test Loss: 7319.210250000\n",
      "Epoch 19, Train Loss: 1304.025937500, Test Loss: 7913.026500000\n",
      "Epoch 20, Train Loss: 1259.560828125, Test Loss: 6265.535500000\n",
      "Epoch 21, Train Loss: 1173.435773438, Test Loss: 6067.384250000\n",
      "Epoch 22, Train Loss: 1134.268062500, Test Loss: 6085.213750000\n",
      "Epoch 23, Train Loss: 1102.335695313, Test Loss: 5714.004000000\n",
      "Epoch 24, Train Loss: 1062.286929688, Test Loss: 5414.306500000\n",
      "Epoch 25, Train Loss: 1069.235882812, Test Loss: 5712.474750000\n",
      "Epoch 26, Train Loss: 1041.568617187, Test Loss: 4264.915625000\n",
      "Epoch 27, Train Loss: 946.501187500, Test Loss: 4258.586750000\n",
      "Epoch 28, Train Loss: 994.763562500, Test Loss: 4633.621000000\n",
      "Epoch 29, Train Loss: 1001.261320312, Test Loss: 5189.330750000\n",
      "Epoch 30, Train Loss: 974.935804687, Test Loss: 4438.352250000\n",
      "Epoch 31, Train Loss: 917.767726563, Test Loss: 3720.247250000\n",
      "Epoch 32, Train Loss: 894.993226563, Test Loss: 4237.198125000\n",
      "Epoch 33, Train Loss: 902.004421875, Test Loss: 3706.422500000\n",
      "Epoch 34, Train Loss: 860.260062500, Test Loss: 3544.265375000\n",
      "Epoch 35, Train Loss: 821.721335938, Test Loss: 3268.391750000\n",
      "Epoch 36, Train Loss: 844.995898438, Test Loss: 3672.302750000\n",
      "Epoch 37, Train Loss: 825.060414062, Test Loss: 3439.837875000\n",
      "Epoch 38, Train Loss: 806.043531250, Test Loss: 4027.758125000\n",
      "Epoch 39, Train Loss: 796.112195313, Test Loss: 3329.107375000\n",
      "Epoch 40, Train Loss: 773.378968750, Test Loss: 3391.958250000\n",
      "Epoch 41, Train Loss: 761.324242187, Test Loss: 3670.858000000\n",
      "Epoch 42, Train Loss: 795.425867187, Test Loss: 3491.788375000\n",
      "Epoch 43, Train Loss: 739.072968750, Test Loss: 3674.684625000\n",
      "Epoch 44, Train Loss: 769.123640625, Test Loss: 3598.059625000\n",
      "Epoch 45, Train Loss: 757.648046875, Test Loss: 3110.581875000\n",
      "Epoch 46, Train Loss: 774.877429688, Test Loss: 3253.814500000\n",
      "Epoch 47, Train Loss: 726.135109375, Test Loss: 3253.214125000\n",
      "Epoch 48, Train Loss: 704.930156250, Test Loss: 2817.201250000\n",
      "Epoch 49, Train Loss: 666.907109375, Test Loss: 2851.457375000\n",
      "Epoch 50, Train Loss: 666.750015625, Test Loss: 2949.007000000\n",
      "Epoch 51, Train Loss: 634.953015625, Test Loss: 2537.999750000\n",
      "Epoch 52, Train Loss: 605.819757813, Test Loss: 2644.491250000\n",
      "Epoch 53, Train Loss: 604.002468750, Test Loss: 2466.742875000\n",
      "Epoch 54, Train Loss: 593.321777344, Test Loss: 2452.706125000\n",
      "Epoch 55, Train Loss: 586.952449219, Test Loss: 2290.766500000\n",
      "Epoch 56, Train Loss: 578.449316406, Test Loss: 2358.045875000\n",
      "Epoch 57, Train Loss: 579.717484375, Test Loss: 2346.022000000\n",
      "Epoch 58, Train Loss: 569.907023438, Test Loss: 2311.736500000\n",
      "Epoch 59, Train Loss: 561.992089844, Test Loss: 2279.189125000\n",
      "Epoch 60, Train Loss: 549.618886719, Test Loss: 2206.274875000\n",
      "Epoch 61, Train Loss: 554.488410156, Test Loss: 2326.244500000\n",
      "Epoch 62, Train Loss: 558.315164063, Test Loss: 2355.374500000\n",
      "Epoch 63, Train Loss: 552.146031250, Test Loss: 2246.104062500\n",
      "Epoch 64, Train Loss: 548.377312500, Test Loss: 2280.106500000\n",
      "Epoch 65, Train Loss: 545.526519531, Test Loss: 2228.440625000\n",
      "Epoch 66, Train Loss: 529.782472656, Test Loss: 2117.714312500\n",
      "Epoch 67, Train Loss: 518.666156250, Test Loss: 2147.456375000\n",
      "Epoch 68, Train Loss: 526.939378906, Test Loss: 2282.886937500\n",
      "Epoch 69, Train Loss: 541.360558594, Test Loss: 2105.872875000\n",
      "Epoch 70, Train Loss: 526.782023438, Test Loss: 2334.633000000\n",
      "Epoch 71, Train Loss: 526.420015625, Test Loss: 2295.198125000\n",
      "Epoch 72, Train Loss: 513.414851562, Test Loss: 2137.017250000\n",
      "Epoch 73, Train Loss: 501.572074219, Test Loss: 2197.181000000\n",
      "Epoch 74, Train Loss: 513.233382813, Test Loss: 2176.163562500\n",
      "Epoch 75, Train Loss: 509.509410156, Test Loss: 2036.639437500\n",
      "Epoch 76, Train Loss: 505.540042969, Test Loss: 2172.437750000\n",
      "Epoch 77, Train Loss: 513.260738281, Test Loss: 2064.951750000\n",
      "Epoch 78, Train Loss: 498.504035156, Test Loss: 2027.521187500\n",
      "Epoch 79, Train Loss: 499.849566406, Test Loss: 2086.562562500\n",
      "Epoch 80, Train Loss: 492.423886719, Test Loss: 2037.149062500\n",
      "Epoch 81, Train Loss: 488.726496094, Test Loss: 2011.915062500\n",
      "Epoch 82, Train Loss: 485.105449219, Test Loss: 1962.760562500\n",
      "Epoch 83, Train Loss: 479.502691406, Test Loss: 2099.625625000\n",
      "Epoch 84, Train Loss: 487.805582031, Test Loss: 2121.983062500\n",
      "Epoch 85, Train Loss: 480.725347656, Test Loss: 2181.221187500\n",
      "Epoch 86, Train Loss: 481.465878906, Test Loss: 2047.167875000\n",
      "Epoch 87, Train Loss: 484.220792969, Test Loss: 2023.989937500\n",
      "Epoch 88, Train Loss: 481.493695312, Test Loss: 1986.103000000\n",
      "Epoch 89, Train Loss: 485.169105469, Test Loss: 2062.094875000\n",
      "Epoch 90, Train Loss: 472.243554688, Test Loss: 2052.378250000\n",
      "Epoch 91, Train Loss: 476.457660156, Test Loss: 2081.191875000\n",
      "Epoch 92, Train Loss: 472.581855469, Test Loss: 1990.301875000\n",
      "Epoch 93, Train Loss: 465.442824219, Test Loss: 2007.316625000\n",
      "Epoch 94, Train Loss: 466.091878906, Test Loss: 1965.904562500\n",
      "Epoch 95, Train Loss: 460.159031250, Test Loss: 1966.976812500\n",
      "Epoch 96, Train Loss: 456.962871094, Test Loss: 1932.146625000\n",
      "Epoch 97, Train Loss: 455.828496094, Test Loss: 1961.951062500\n",
      "Epoch 98, Train Loss: 454.434089844, Test Loss: 1869.494937500\n",
      "Epoch 99, Train Loss: 455.152457031, Test Loss: 1961.905062500\n",
      "Epoch 100, Train Loss: 463.056925781, Test Loss: 1904.150312500\n",
      "Epoch 101, Train Loss: 450.168667969, Test Loss: 1877.937312500\n",
      "Epoch 102, Train Loss: 437.348730469, Test Loss: 1873.800812500\n",
      "Epoch 103, Train Loss: 436.580269531, Test Loss: 1747.542000000\n",
      "Epoch 104, Train Loss: 421.797398438, Test Loss: 1783.186500000\n",
      "Epoch 105, Train Loss: 421.990335937, Test Loss: 1703.300500000\n",
      "Epoch 106, Train Loss: 422.195511719, Test Loss: 1827.983312500\n",
      "Epoch 107, Train Loss: 421.063167969, Test Loss: 1723.863437500\n",
      "Epoch 108, Train Loss: 413.192652344, Test Loss: 1687.521937500\n",
      "Epoch 109, Train Loss: 411.474664062, Test Loss: 1674.115750000\n",
      "Epoch 110, Train Loss: 410.181015625, Test Loss: 1675.483625000\n",
      "Epoch 111, Train Loss: 413.259050781, Test Loss: 1704.450937500\n",
      "Epoch 112, Train Loss: 408.000535156, Test Loss: 1696.390375000\n",
      "Epoch 113, Train Loss: 407.258847656, Test Loss: 1660.872000000\n",
      "Epoch 114, Train Loss: 400.185910156, Test Loss: 1648.193500000\n",
      "Epoch 115, Train Loss: 403.350929688, Test Loss: 1674.938562500\n",
      "Epoch 116, Train Loss: 397.311851563, Test Loss: 1666.735625000\n",
      "Epoch 117, Train Loss: 397.523878906, Test Loss: 1684.176500000\n",
      "Epoch 118, Train Loss: 400.813292969, Test Loss: 1616.520187500\n",
      "Epoch 119, Train Loss: 396.616500000, Test Loss: 1615.848187500\n",
      "Epoch 120, Train Loss: 390.620347656, Test Loss: 1649.415187500\n",
      "Epoch 121, Train Loss: 391.693859375, Test Loss: 1591.334812500\n",
      "Epoch 122, Train Loss: 387.713648438, Test Loss: 1574.262062500\n",
      "Epoch 123, Train Loss: 388.787957031, Test Loss: 1593.166125000\n",
      "Epoch 124, Train Loss: 386.602726562, Test Loss: 1576.915062500\n",
      "Epoch 125, Train Loss: 385.282960937, Test Loss: 1638.508250000\n",
      "Epoch 126, Train Loss: 395.592210937, Test Loss: 1608.054500000\n",
      "Epoch 127, Train Loss: 384.290488281, Test Loss: 1606.669000000\n",
      "Epoch 128, Train Loss: 391.081468750, Test Loss: 1568.346375000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129, Train Loss: 381.635847656, Test Loss: 1563.037312500\n",
      "Epoch 130, Train Loss: 382.032675781, Test Loss: 1539.096500000\n",
      "Epoch 131, Train Loss: 379.192421875, Test Loss: 1553.957750000\n",
      "Epoch 132, Train Loss: 377.112667969, Test Loss: 1556.923687500\n",
      "Epoch 135, Train Loss: 375.436984375, Test Loss: 1490.374687500\n",
      "Epoch 136, Train Loss: 372.017996094, Test Loss: 1520.593562500\n",
      "Epoch 137, Train Loss: 368.706632813, Test Loss: 1574.937125000\n",
      "Epoch 138, Train Loss: 368.380988281, Test Loss: 1554.281687500\n",
      "Epoch 139, Train Loss: 371.378382812, Test Loss: 1526.620500000\n",
      "Epoch 140, Train Loss: 365.499691406, Test Loss: 1510.476687500\n",
      "Epoch 141, Train Loss: 367.222371094, Test Loss: 1495.295875000\n",
      "Epoch 142, Train Loss: 362.685058594, Test Loss: 1493.099000000\n",
      "Epoch 143, Train Loss: 359.699132813, Test Loss: 1503.631125000\n",
      "Epoch 144, Train Loss: 359.760210938, Test Loss: 1457.417625000\n",
      "Epoch 145, Train Loss: 359.322304687, Test Loss: 1469.909687500\n",
      "Epoch 146, Train Loss: 355.793535156, Test Loss: 1460.786250000\n",
      "Epoch 147, Train Loss: 361.050863281, Test Loss: 1434.090062500\n",
      "Epoch 148, Train Loss: 353.316523437, Test Loss: 1465.393812500\n",
      "Epoch 149, Train Loss: 358.698617188, Test Loss: 1443.501687500\n",
      "Time to fit model 1 :  2:55:20.509326\n",
      "KLD: 45.709938049316406\n",
      "Epoch 0, Train Loss: 3907.293093750, Test Loss: 16167.053000000\n",
      "Epoch 1, Train Loss: 3090.795593750, Test Loss: 14287.469000000\n",
      "Epoch 2, Train Loss: 2494.984265625, Test Loss: 17191.828000000\n",
      "Epoch 3, Train Loss: 2137.396875000, Test Loss: 21489.436000000\n",
      "Epoch 4, Train Loss: 1965.215765625, Test Loss: 17216.260000000\n",
      "Epoch 5, Train Loss: 1867.156203125, Test Loss: 26448.059000000\n",
      "Epoch 6, Train Loss: 1814.446390625, Test Loss: 25264.616000000\n",
      "Epoch 7, Train Loss: 1787.274218750, Test Loss: 24387.228000000\n",
      "Epoch 8, Train Loss: 1730.393906250, Test Loss: 15292.639000000\n",
      "Epoch 9, Train Loss: 1692.026015625, Test Loss: 14389.074000000\n",
      "Epoch 10, Train Loss: 1663.551046875, Test Loss: 12974.533000000\n",
      "Epoch 11, Train Loss: 1623.258734375, Test Loss: 12893.660500000\n",
      "Epoch 12, Train Loss: 1589.053515625, Test Loss: 12716.272000000\n",
      "Epoch 13, Train Loss: 1572.542343750, Test Loss: 11108.382500000\n",
      "Epoch 14, Train Loss: 1548.961906250, Test Loss: 11320.552500000\n",
      "Epoch 15, Train Loss: 1517.637312500, Test Loss: 9639.871500000\n",
      "Epoch 16, Train Loss: 1451.854500000, Test Loss: 8125.730000000\n",
      "Epoch 17, Train Loss: 1417.291406250, Test Loss: 6967.546500000\n",
      "Epoch 18, Train Loss: 1319.370062500, Test Loss: 7312.862750000\n",
      "Epoch 19, Train Loss: 1267.334648437, Test Loss: 6825.976500000\n",
      "Epoch 20, Train Loss: 1293.304437500, Test Loss: 5626.499500000\n",
      "Epoch 21, Train Loss: 1210.637781250, Test Loss: 5492.349500000\n",
      "Epoch 22, Train Loss: 1201.922625000, Test Loss: 5173.800250000\n",
      "Epoch 23, Train Loss: 1107.092000000, Test Loss: 5383.754000000\n",
      "Epoch 24, Train Loss: 1050.450406250, Test Loss: 4714.090750000\n",
      "Epoch 25, Train Loss: 1021.188671875, Test Loss: 4825.211000000\n",
      "Epoch 26, Train Loss: 1016.906851563, Test Loss: 5126.954750000\n",
      "Epoch 27, Train Loss: 1027.698914063, Test Loss: 4305.063500000\n",
      "Epoch 28, Train Loss: 990.409765625, Test Loss: 4248.098375000\n",
      "Epoch 29, Train Loss: 975.401828125, Test Loss: 4683.304750000\n",
      "Epoch 30, Train Loss: 954.304609375, Test Loss: 4447.293000000\n",
      "Epoch 31, Train Loss: 895.206515625, Test Loss: 3557.638625000\n",
      "Epoch 32, Train Loss: 879.735789062, Test Loss: 3928.450000000\n",
      "Epoch 33, Train Loss: 904.610937500, Test Loss: 4599.982500000\n",
      "Epoch 34, Train Loss: 964.351007813, Test Loss: 4688.335500000\n",
      "Epoch 35, Train Loss: 920.007156250, Test Loss: 3903.215875000\n",
      "Epoch 36, Train Loss: 871.486656250, Test Loss: 3590.796625000\n",
      "Epoch 37, Train Loss: 830.778757812, Test Loss: 4023.822375000\n",
      "Epoch 38, Train Loss: 820.627945312, Test Loss: 3416.989125000\n",
      "Epoch 39, Train Loss: 781.682468750, Test Loss: 3642.609250000\n",
      "Epoch 40, Train Loss: 802.743976563, Test Loss: 3992.597250000\n",
      "Epoch 41, Train Loss: 862.698578125, Test Loss: 3181.576000000\n",
      "Epoch 42, Train Loss: 759.205148438, Test Loss: 3491.197875000\n",
      "Epoch 43, Train Loss: 755.152125000, Test Loss: 3342.827875000\n",
      "Epoch 44, Train Loss: 776.034406250, Test Loss: 3061.435375000\n",
      "Epoch 45, Train Loss: 749.753015625, Test Loss: 3015.899500000\n",
      "Epoch 46, Train Loss: 742.377484375, Test Loss: 3007.354625000\n",
      "Epoch 47, Train Loss: 716.449851563, Test Loss: 3043.099125000\n",
      "Epoch 48, Train Loss: 721.710859375, Test Loss: 2837.567500000\n",
      "Epoch 49, Train Loss: 707.778570313, Test Loss: 2893.996875000\n",
      "Epoch 50, Train Loss: 702.391742187, Test Loss: 2789.023625000\n",
      "Epoch 51, Train Loss: 658.828296875, Test Loss: 2608.630000000\n",
      "Epoch 52, Train Loss: 632.465789062, Test Loss: 2632.745375000\n",
      "Epoch 53, Train Loss: 633.409578125, Test Loss: 2587.932500000\n",
      "Epoch 54, Train Loss: 623.334457031, Test Loss: 2604.074625000\n",
      "Epoch 55, Train Loss: 611.381570313, Test Loss: 2532.259750000\n",
      "Epoch 56, Train Loss: 601.719968750, Test Loss: 2539.883125000\n",
      "Epoch 57, Train Loss: 594.987402344, Test Loss: 2456.877750000\n",
      "Epoch 58, Train Loss: 590.587832031, Test Loss: 2438.575125000\n",
      "Epoch 59, Train Loss: 590.733476562, Test Loss: 2501.700875000\n",
      "Epoch 60, Train Loss: 582.894238281, Test Loss: 2435.369875000\n",
      "Epoch 61, Train Loss: 579.518363281, Test Loss: 2385.101875000\n",
      "Epoch 62, Train Loss: 571.780781250, Test Loss: 2472.157750000\n",
      "Epoch 63, Train Loss: 572.586722656, Test Loss: 2398.391625000\n",
      "Epoch 64, Train Loss: 570.367023438, Test Loss: 2382.811750000\n",
      "Epoch 65, Train Loss: 569.201175781, Test Loss: 2377.839812500\n",
      "Epoch 66, Train Loss: 565.895214844, Test Loss: 2315.409875000\n",
      "Epoch 67, Train Loss: 552.617261719, Test Loss: 2316.866625000\n",
      "Epoch 68, Train Loss: 553.331832031, Test Loss: 2264.786312500\n",
      "Epoch 69, Train Loss: 549.245320312, Test Loss: 2272.973625000\n",
      "Epoch 70, Train Loss: 548.933699219, Test Loss: 2258.716312500\n",
      "Epoch 71, Train Loss: 541.236058594, Test Loss: 2309.824000000\n",
      "Epoch 72, Train Loss: 544.959414062, Test Loss: 2293.961500000\n",
      "Epoch 73, Train Loss: 544.141601562, Test Loss: 2301.348875000\n",
      "Epoch 74, Train Loss: 545.629476563, Test Loss: 2252.679875000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "emb_dim = 32\n",
    "latent_dim = 32\n",
    "dpout = .1\n",
    "kappa = 500\n",
    "pool = 'max'\n",
    "    \n",
    "### train the model(s)\n",
    "for frac in [1. , 0.]:\n",
    "    for k in range(1):\n",
    "        start = datetime.now()\n",
    "        # model\n",
    "        enc = SeqEncoder(INPUT_DIM, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "        dec = SeqDecoder(INPUT_DIM, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "        dist = vMF(latent_dim, kappa=kappa)\n",
    "        model = Seq2Seq_VAE(enc, dec, dist, device).to(device)\n",
    "        classifier = PoolingClassifier(latent_dim, NUM_CLASSES, n_walks,dpout,pooling=pool).to(device)\n",
    "        \n",
    "        # losses\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "        mse_loss = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "        suffix = 'emb%i_hid%i_lat%i_dp%.1f_k%i_%s'%(emb_dim,emb_dim,latent_dim,dpout,kappa,pool)\n",
    "        suffix += '_frac%.1f'%frac\n",
    "        suffix += '_scaled'\n",
    "        suffix += '_sum'\n",
    "        save_path_model = './models/%s/%s_run%i_best.pt'%(folder,suffix,(k+1))\n",
    "        save_path_losses = './models/%s/losses_%s_%i.npy'%(folder, suffix, (k+1))\n",
    "        \n",
    "        if os.path.exists(save_path_model):\n",
    "            \n",
    "            # load model and train further\n",
    "            state_dict = torch.load(save_path_model)\n",
    "            model.load_state_dict(state_dict['model_state_dict'])\n",
    "            classifier.load_state_dict(state_dict['classifier_state_dict'])\n",
    "            \n",
    "            optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()))\n",
    "            optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "            # half the learning rate to be consistent with the other training\n",
    "            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr']/2\n",
    "            \n",
    "            losses = np.load(save_path_losses)\n",
    "            best_test_loss = losses[state_dict['epoch'],2]\n",
    "            training = list(losses[:,:2])\n",
    "            validation = list(losses[:,2:])\n",
    "            last_epoch = losses.shape[0]\n",
    "            \n",
    "        else:\n",
    "            # initialize model\n",
    "            model.apply(init_weights)\n",
    "            classifier.apply(init_weights)\n",
    "\n",
    "\n",
    "            #optimizer\n",
    "            optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "\n",
    "            best_test_loss = np.infty\n",
    "            training = []\n",
    "            validation = []\n",
    "       \n",
    "            last_epoch = 0\n",
    "            \n",
    "        for e in range(last_epoch, N_EPOCHS):\n",
    "\n",
    "            train_loss, train_class_loss = train(model, classifier, train_iterator, optimizer, \n",
    "                                               calculate_loss,cross_entropy_loss, \n",
    "                                                 clip=1,norm_p=None, class_fraction=frac)\n",
    "            val_loss, val_class_loss = evaluate(model,classifier, val_iterator,\n",
    "                                                 calculate_loss, cross_entropy_loss, norm_p=None)\n",
    "\n",
    "\n",
    "            train_loss /= N_train\n",
    "            train_class_loss /= N_train\n",
    "            val_loss /= N_val\n",
    "            val_class_loss /=N_val\n",
    "\n",
    "            training += [[train_loss, train_class_loss]]\n",
    "            validation += [[val_loss, val_class_loss]]\n",
    "            print(f'Epoch {e}, Train Loss: {train_loss:.9f}, Test Loss: {val_loss:.9f}')\n",
    "\n",
    "\n",
    "            if e % 50 == 0 and e > 0 :\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr']/2\n",
    "\n",
    "            if best_test_loss > val_loss:\n",
    "                best_test_loss = val_loss\n",
    "                end = datetime.now()\n",
    "                torch.save({'epoch': e,\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'classifier_state_dict': classifier.state_dict()\n",
    "                               }, save_path_model)\n",
    "                # save training and validation loss\n",
    "                validation_ = np.array(validation)\n",
    "                training_ = np.array(training)\n",
    "                losses = np.concatenate((training_, validation_), axis=1)\n",
    "                # losses [:,0] = training loss, [:,1] = training classification loss\n",
    "                # [:,2] = validation loss, [:,3] = validation classification loss\n",
    "                with open(save_path_losses, 'wb') as f:\n",
    "                    np.save(f,losses)\n",
    "\n",
    "        validation_ = np.array(validation)\n",
    "        training_ = np.array(training)\n",
    "        losses = np.concatenate((training_, validation_), axis=1)\n",
    "        # losses [:,0] = training loss, [:,1] = training classification loss\n",
    "        # [:,2] = validation loss, [:,3] = validation classification loss\n",
    "        with open(save_path_losses, 'wb') as f:\n",
    "            np.save(f,losses)\n",
    "        end = datetime.now()\n",
    "        print('Time to fit model %i : '%(k+1), end-start)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/5_populations/losses_emb32_hid32_lat32_dp0.1_k500_max_frac0.0_scaled_sum_1.npy', 'rb') as f:\n",
    "    l = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Rec loss')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l[:,2]- l[:,3])\n",
    "sns.despine()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Rec loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
